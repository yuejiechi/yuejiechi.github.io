
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
<script type="text/javascript">LatexIT.add('li',true);</script>
<title>Yuejie Chi</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><img class="menu" src="yale_logo.png" width="100px" align="center"></div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="publications.html" class="current">Papers</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="news.html">News</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yuejie Chi</h1>
</div>

<p>You can find representative papers categorized by topics under <a href="research.html">Research</a>. My <a href="https://scholar.google.com/citations?hl=en&user=h1NXfKYAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> and <a href="https://arxiv.org/search/?query=yuejie+chi&searchtype=author">Arxiv</a> profiles sometimes are more up-to-date. Accompanying codes can be downloaded next to the link of the papers when available.

<br>
<br clear="all" />


<div id="preprints">
<h2> Preprints </h2>

<br>

<ul>

<li><p><a href="https://arxiv.org/abs/2504.14439">LoRe: Personalizing LLMs
via Low-Rank Reward Modeling</a> <a href="https://arxiv.org/abs/2504.14439">[Arxiv]</a>
<br>A. Bose, Z. Xiong, Y. Chi, S. Du, L. Xiao, and M. Fazel, <i>preprint</i>. </p> 

<li><p><a href="https://arxiv.org/abs/2505.07861">Scalable LLM Math Reasoning Acceleration with Low-rank Distillation</a> <a href="https://arxiv.org/abs/2505.07861">[Arxiv]</a>
<br>H. Dong, B. Acun, B Chen, and Y. Chi, <i>preprint</i>. </p> 

<li><p><a href="https://arxiv.org/abs/2408.02320">A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion Models</a> <a href="https://arxiv.org/abs/2408.02320">[Arxiv]</a>
<br>G. Li, Y. Wei, Y. Chi, and Y. Chen, <i>preprint</i>.

<li><p><a href="https://arxiv.org/pdf/2402.02698.pdf">Beyond Expectations: Learning with Stochastic Dominance Made Practical</a> <a href="https://arxiv.org/pdf/2402.02698.pdf">[Arxiv]</a>
<br>S. Cen, J. Mei, H. Dai, D. Schuurmans, Y. Chi, and B. Dai, <i>preprint</i>.   </p> 

<li><p><a href="https://arxiv.org/abs/2309.09977">A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning</a> <a href="https://arxiv.org/abs/2309.09977">[Arxiv]</a>
<br>P. Valdeira, Y. Chi, C. Soares, and J. Xavier, <i>preprint</i>. Short version at NeurIPS 2022 FL Workshop.

</ul>  

<br>


<div id="monographs">
<h2> Monographs and Overview Articles </h2>

<br>

<ul>

<li> <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-66497-7_7">Provably Accelerating Ill-Conditioned
Low-Rank Estimation via Scaled Gradient Descent, Even with Overparameterization</a> <a href="https://arxiv.org/pdf/2310.06159.pdf">[Arxiv]</a> 
<br> C. Ma, X. Xu, T. Tong and Y. Chi, <i>Explorations in the Mathematics of Data Science</i>, Springer, pp. 133-165, 2024.</p>

<li><p><a href="https://www.nowpublishers.com/article/Details/MAL-079">Spectral Methods for Data Science: A Statistical Perspective</a> <a href="https://arxiv.org/abs/2012.08496">[Arxiv]</a>
<br> Y. Chen, Y. Chi, J. Fan, and C. Ma, <i>Foundation and Trends in Machine Learning</i>, vol. 14, no. 5, pp. 566-806, 2021. </p> 

<li> <p><a href="https://doi.org/10.1109/MSP.2019.2962209">Harnessing Sparsity over the Continuum: 
Atomic Norm Minimization for Super Resolution</a> <a href="https://arxiv.org/pdf/1904.04283.pdf">[Arxiv]</a> <a href="code/AtomicNorm_PerformanceComparison.zip">[Code]</a>
<br>Y. Chi and M. Ferreira Da Costa, <i>IEEE Signal Processing Magazine</i>, vol. 37, no. 2, pp. 39-57, 2020.</p>

<li> <p><a href="http://doi.org/10.1109/TSP.2019.2937282">Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview</a> <a href="https://arxiv.org/pdf/1809.09573.pdf">[Arxiv]</a> <a href="talks/Nonconvex_overview_slides.pdf">[Slides]</a> 
<br> Y. Chi, Y. M. Lu, and Y. Chen, <i>IEEE Trans. on Signal Processing</i>, vol. 67, no. 20, pp. 5239-5269, 2019.</p>

<li> <p><a href="https://link.springer.com/chapter/10.1007/978-3-319-73074-5_8">Median-Truncated Gradient Descent: A Robust and Scalable Nonconvex Approach for Signal Estimation</a> 
<br>Y. Chi, Y. Li, H. Zhang, and Y. Liang, <i>Compressed Sensing and Its Applications</i>, Springer, Birkhauser, 2019.</p>

<li> <p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8417980">Streaming PCA and Subspace Tracking: The Missing Data Case</a> <a href="https://arxiv.org/pdf/1806.04609.pdf">[Arxiv]</a> <a href="https://gitlab.eecs.umich.edu/girasole/ProcIEEEstreamingPCA/">[Code]</a>
<br> L. Balzano, Y. Chi, and Y. M. Lu, <i>Proceedings of the IEEE</i>, vol. 106, no. 8, pp. 1293-1310, 2018.</p>

<li> <p><a href="https://ieeexplore.ieee.org/document/8399563">Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation</a> <a href="https://arxiv.org/pdf/1802.08397.pdf">[Arxiv]</a>
<br> Y. Chen and Y. Chi, <i>IEEE Signal Processing Magazine</i>, vol. 35, no. 4, pp. 14-31, 2018. </p>

<li> <p><a href="http://link.springer.com/chapter/10.1007/978-3-319-16042-9_3">Compressed Sensing, Sparse Inversion, and Model Mismatch</a>
<br>A. Pezeshki, Y. Chi, L. L. Scharf, and E. K. Chong. <i>Compressed Sensing and Its Applications</i>, Birkhauser, 2015. </p>

</ul>

<br>



<div id="journal">
<h2> Published </h2>

<br>

<ul>

<h3> 2025+ </h3> 

<li><p><a href="https://arxiv.org/abs/2301.13006">Fast Computation of Optimal Transport via Entropy-Regularized Extragradient Methods</a> <a href="https://arxiv.org/abs/2301.13006">[Arxiv]</a>
<br>G. Li, Y. Chen, Y. Huang, Y. Chi, H. V. Poor, and Y. Chen, <i>SIAM Journal on Optimization</i>, accepted. </p> 

<li><p><a href="https://jmlr.org/papers/volume26/24-0579/24-0579.pdf">The Blessing of Heterogeneity in Federated Q-Learning: Linear Speedup and Beyond</a> <a href="https://arxiv.org/abs/2305.10697">[Arxiv]</a>
<br>J. Woo, G. Joshi, and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 26, no. 26, pp. 1-85, 2025. Short version at ICML 2023. </p> 

<li><p><a href="https://doi.org/10.1109/TSP.2025.3540655">Communication-efficient Vertical Federated Learning via Compressed Error Feedback</a> <a href="https://arxiv.org/abs/2406.14420">[Arxiv]</a> 
<br>P. Valdeira, J. Xavier, C. Soares, and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 73, pp. 1065-1080, 2025. Short version at EUSIPCO 2024 as an <b>invited</b> paper.  </p> 
   
<li><p><a href="https://doi.org/10.1109/JSTSP.2025.3526081">Convergence and Privacy of Decentralized Nonconvex Optimization with Gradient Clipping and Communication Compression</a> <a href="https://arxiv.org/abs/2305.09896">[Arxiv]</a>
<br>B. Li and Y. Chi, <i>IEEE Journal of Selected Topics in Signal Processing</i>, vol. 19, no. 1, pp. 273-282, 2025.  </p> 

<li><p><a href="https://doi.org/10.1109/TSIPN.2025.3539004">Communication-Efficient Federated Optimization over Semi-Decentralized Networks</a> <a href="https://arxiv.org/abs/2311.18787">[Arxiv]</a>  
<br>H. Wang and Y. Chi, <i>IEEE Trans. on Signal and Information Processing over Networks</i>, vol. 11, pp. 147-160, 2025. Short version at ICASSP 2024.  </p> 

<li><p><a href="https://openreview.net/forum?id=ciUHD7jcT9">Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games</a> <a href="http://arxiv.org/abs/2502.09780">[Arxiv]</a>
<br>T. Yang, B. Dai, L. Xiao, and Y. Chi, <i>International Conference on Machine Learning (ICML)</i>, 2025. </p> 

<li><p><a href="https://openreview.net/forum?id=fFBnsYRsPg">Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement Learning</a> <a href="https://arxiv.org/abs/2409.20067">[Arxiv]</a>
<br>L. Shi*, J. Gai*, E. Mazumdar, Y. Chi, and A. Wierman, <i>International Conference on Machine Learning (ICML)</i>, 2025. </p> 

<li><p><a href="https://openreview.net/forum?id=oa7MYAO6h6">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a> <a href="https://arxiv.org/abs/2410.21465">[Arxiv]</a> <a href="https://github.com/bytedance/ShadowKV">[Code]</a>
<br>H. Sun, L.-W. Chang, W. Bao, S. Zheng, N. Zheng, X. Liu, H. Dong, Y. Chi, and B. Chen, <i>International Conference on Machine Learning (ICML)</i>, 2025, <b>spotlight</b> presentation. </p> 

<li><p><a href="https://openreview.net/forum?id=JOAAvVi1pf">Faster WIND: Accelerating Iterative Best-of-N Distillation for LLM Alignment</a> <a href="https://arxiv.org/abs/2410.20727">[Arxiv]</a>
<br>T. Yang, J. Mei, H. Dai, Z. Wen, S. Cen, D. Schuurmans, Y. Chi, and B. Dai, <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2025. </p>  

<li><p><a href="https://openreview.net/forum?id=9bB1FJSKKS">Characterizing the Accuracy-Communication-Privacy Trade-off in Distributed Stochastic Convex Optimization</a> <a href="https://arxiv.org/abs/2501.03222">[Arxiv]</a>
<br>S. Salgia, N. Pavlovic, Y. Chi and Q. Zhao, <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2025. </p> 

<li><p><a href="https://openreview.net/forum?id=2uQBSa2X4R">Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning</a> <a href="https://arxiv.org/abs/2502.19652">[Arxiv]</a> <a href="https://robust-rl.com/">[Website]</a> <a href="https://github.com/SafeRL-Lab/Robust-Gymnasium">[Code]</a>
<br>S. Gu*, L Shi*, M. Wen, M. Jin, E. Mazumdar, Y. Chi, A. Wierman, C. Spanos, <i>International Conference on Learning Representations (ICLR)</i>, 2025. </p>  

<li><p><a href="https://openreview.net/forum?id=SQnitDuow6">Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF</a> <a href="https://arxiv.org/abs/2405.19320">[Arxiv]</a>
<br>S. Cen, J. Mei, K. Goshvadi, H. Dai, T. Yang, S. Yang, D. Schuurmans, Y. Chi, and B. Dai, <i>International Conference on Learning Representations (ICLR)</i>, 2025. </p>   

<li><p><a href="https://openreview.net/forum?id=OXi1FmHGzz">Vertical Federated Learning with Missing Features During Training and Inference</a> <a href="https://arxiv.org/abs/2410.22564">[Arxiv]</a>
<br>P. Valdeira, S. Wang, and Y. Chi, <i>International Conference on Learning Representations (ICLR)</i>, 2025. </p> 

<li><p><a href="https://openreview.net/forum?id=Antib6Uovh">A Theoretical Analysis of Self-Supervised Learning for Vision Transformers</a> <a href="https://arxiv.org/abs/2403.02233">[Arxiv]</a>
<br>Y. Huang*, Z. Wen*, Y. Chi, and Y. Liang, <i>International Conference on Learning Representations (ICLR)</i>, 2025.</p> 

<li><p><a href="https://doi.org/10.1109/ICASSP49660.2025.10889036">Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information</a> <a href="https://arxiv.org/abs/2410.05143">[Arxiv]</a>
<br>T. Efimov, H. Dong, M. Shah, J. Simmons, S. Donegan, and Y. Chi, <i>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2025. </p>


<h3> 2024 </h3> 

<li><p><a href="https://jmlr.org/papers/volume25/22-1482/22-1482.pdf">Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity</a> <a href="https://arxiv.org/abs/2208.05767">[Arxiv]</a> <a href="https://github.com/Laixishi/Robust-RL-with-KL-divergence">[Code]</a>
<br>L. Shi and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 25, no. 200, pp. 1-91, 2024.  </p> 

<li> <p><a href="https://doi.org/10.1214/23-AOS2342">Settling the Sample Complexity of Model-Based Offline Reinforcement Learning</a> <a href="https://arxiv.org/pdf/2204.05275.pdf">[Arxiv]</a> 
<br>G. Li, L. Shi, Y. Chen, Y. Chi and Y. Wei, <i>The Annals of Statistics</i>, vol. 52, no. 1, pp. 233-260, 2024.  </p> 

<li><p><a href="https://jmlr.org/papers/volume25/21-1205/21-1205.pdf">Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization</a> <a href="https://arxiv.org/pdf/2105.15186.pdf">[Arxiv]</a> 
<br>S. Cen, Y. Wei, and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 25, no. 4, pp. 1-48, 2024. Short version at NeurIPS 2021.</p> 

<li> <p><a href="https://pubsonline.informs.org/doi/full/10.1287/opre.2023.2450">Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis</a> <a href="https://arxiv.org/pdf/2102.06548.pdf">[Arxiv]</a> 
<br>G. Li, C. Cai, Y. Chen, Y. Wei, and Y. Chi, <i>Operations Research</i>, vol. 72, no. 1, pp. 222-236, 2024. Short version at ICML 2021. </p> 

 <li> <p><a href="https://pubsonline.informs.org/doi/full/10.1287/opre.2023.2451">Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model</a> <a href="https://arxiv.org/pdf/2005.12900.pdf">[Arxiv]</a>  
<br>G. Li, Y. Wei, Y. Chi, and Y. Chen, <i>Operations Research</i>, vol. 72, no. 1, pp. 203-221, 2024. Short version at NeurIPS 2020. </p> 

<li><p><a href="https://rlj.cs.umass.edu/2024/papers/Paper189.html">Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes</a> <a href="https://arxiv.org/abs/2403.12946">[Arxiv]</a>
<br>H. Wang, L. Shi, and Y. Chi, <i>Reinforcement Learning Journal</i>, vol. 3, pp. 1467-1510, 2024. </p>

<li><p><a href="https://doi.org/10.1109/TIT.2024.3394685">High-probability Sample Complexities for Policy Evaluation with Linear Function Approximation</a> <a href="https://arxiv.org/abs/2305.19001">[Arxiv]</a>
<br>G. Li*, W. Wu*, Y. Chi, C. Ma, A. Rinaldo, and Y. Wei, <i>IEEE Trans. on Information Theory</i>, vol. 70, no. 8, pp. 5969-5999, 2024. 

<li><p><a href="https://openreview.net/forum?id=6YIpvnkjUK">The Sample-Communication Complexity Trade-off in Federated Q-Learning</a> <a href="https://arxiv.org/abs/2408.16981">[Arxiv]</a>
<br>S. Salgia and Y. Chi,  <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2024, <b>oral</b> presentation.  </p> 

<li><p><a href="https://openreview.net/forum?id=ik37kKxKBm">In-Context Learning with Representations: Contextual Generalization of Trained Transformers</a> <a href="https://arxiv.org/abs/2408.10147">[Arxiv]</a>
<br>T. Yang, Y. Huang, Y. Liang, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2024.  </p> 

<li><p><a href="https://openreview.net/forum?id=SLnsoaY4u1">Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction</a> <a href="https://arxiv.org/abs/2403.17042">[Arxiv]</a> <a href="https://github.com/x1xu/diffusion-plug-and-play">[Code]</a>
<br>X. Xu and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2024. </p> 

<li><p><a href="https://openreview.net/forum?id=DUFD6vsyF8">Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning</a> <a href="https://arxiv.org/abs/2311.00201">[Arxiv]</a>
<br>T. Yang, S. Cen, Y. Wei, Y. Chen, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2024. </p> 

<li><p><a href="https://openreview.net/forum?id=bO5bUxvH6m">Learning Discrete Concepts in Latent Hierarchical Models</a> <a href="https://arxiv.org/abs/2406.00519">[Arxiv]</a>
<br>L. Kong, G. Chen, B. Huang, E. Xing, Y. Chi, and K. Zhang, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2024. </p> 

<li><p><a href="https://doi.org/10.1109/GLOBECOM52923.2024.10901010">Scalable Dynamic Resource Allocation via Domain Randomized Reinforcement Learning</a> 
<br>Y. Wang, L. Shi, M. Lee, J. Sydir, Z. Zhou, Y. Chi, and B. Li, <i>IEEE Global Communications Conference (GLOBECOM)</i>, 2024.

<li><p><a href="https://openreview.net/forum?id=4aqq9xTtih">Prompt-prompted Adaptive Structured Pruning for
Efficient LLM Generation</a> <a href="https://arxiv.org/abs/2404.01365">[Arxiv]</a> <a href="https://github.com/hdong920/GRIFFIN">[Code]</a>
<br>H. Dong, B. Chen, and Y. Chi, <i>Conference on Language Modeling (COLM)</i>, 2024.  

<li><p><a href="https://openreview.net/pdf?id=LIPGadocTe">Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices</a> <a href="https://arxiv.org/abs/2402.05876">[Arxiv]</a>
<br>J. Woo, L. Shi, G. Joshi, and Y. Chi, <i>International Conference on Machine Learning (ICML)</i>, 2024.  </p> 

<li><p><a href="https://openreview.net/pdf?id=qDw4FxMubj">Sample-Efficient Robust Multi-Agent Reinforcement
Learning in the Face of Environmental Uncertainty</a> <a href="https://arxiv.org/abs/2404.18909">[Arxiv]</a>
<br>L. Shi, E. Mazumdar, Y. Chi, and A. Wierman, <i> International Conference on Machine Learning (ICML)</i>, 2024.  </p> 

<li><p><a href="https://openreview.net/pdf?id=uhHDhVKFMW">Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference</a> <a href="https://arxiv.org/abs/2402.09398">[Arxiv]</a> <a href="https://github.com/hdong920/LESS">[Code]</a>
<br>H. Dong, X. Yang, Z. Zhang, Z. Wang, Y. Chi, and B. Chen, <i> International Conference on Machine Learning (ICML)</i>, 2024.  

<li><p><a href="https://openreview.net/pdf?id=KB6slOUQP9">Accelerating Convergence of Score-Based Diffusion Models, Provably</a> <a href="https://arxiv.org/abs/2403.03852">[Arxiv]</a> <a href="https://github.com/TimofeyEfimov/AcceleratedDiffusionSamplers">[Code]</a>
<br>G. Li*, Y. Huang*, T. Efimov, Y. Wei, Y. Chi, and Y. Chen, <i> International Conference on Machine Learning (ICML)</i>, 2024.  

<li><p><a href="https://openreview.net/forum?id=4VGEeER6W9">Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models</a> <a href="https://arxiv.org/abs/2306.09251">[Arxiv]</a>
<br>G. Li, Y. Wei, Y. Chen, and Y. Chi, <i>International Conference on Learning Representations (ICLR)</i>, 2024.

<li><p><a href="https://proceedings.mlr.press/v238/chen24d/chen24d.pdf">Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression</a> <a href="https://arxiv.org/abs/2310.19059">[Arxiv]</a>
<br>S. Chen, Z. Li, and Y. Chi, <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2024. 

<h3> 2023 </h3> 

<li><p><a href="https://www.nature.com/articles/s41598-023-47936-6">A Lightweight Transformer for Faster and Robust EBSD Data Collection</a> <a href="https://arxiv.org/abs/2308.09693">[Arxiv]</a> <a href="https://github.com/hdong920/ebsd_slice_recovery">[Code]</a>
<br>H. Dong, S. Donegan, M. Shah, and Y. Chi, <i>Scientific Reports</i>, vol. 23, pp. 21253, 2023.

<li><p><a href="https://doi.org/10.1093/imaiai/iaad019">Fast and Provable Tensor Robust Principal Component Analysis via Scaled Gradient Descent</a> <a href="https://arxiv.org/abs/2206.09109">[Arxiv]</a> <a href="https://github.com/hdong920/Tensor_RPCA_ScaledGD">[Code]</a>
<br>H. Dong, T. Tong, C. Ma, and Y. Chi, <i>Information and Inference: A Journal of the IMA</i>, vol. 12, no. 3, pp. 1716-1758, 2023.  </p> 

<li><p><a href="https://doi.org/10.1007/s10107-022-01920-6">Softmax Policy Gradient Methods Can Take Exponential Time to Converge</a> <a href="https://arxiv.org/pdf/2102.11270.pdf">[Arxiv]</a> 
<br>G. Li, Y. Wei, Y. Chi, and Y. Chen, <i>Mathematical Programming</i>, vol. 201, pp. 707-802, 2023. Short version at COLT 2021.</p> 

<li><p><a href="https://epubs.siam.org/doi/full/10.1137/21M1456789">Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence</a> <a href="https://arxiv.org/pdf/2105.11066.pdf">[Arxiv]</a>
<br>W. Zhan*, S. Cen*, B. Huang, Y. Chen, J. D. Lee, and Y. Chi, <i>SIAM Journal on Optimization</i>, vol. 33, no. 2, pp. 1061-1091, 2023. Short version at OPT 2021 as an <b>oral</b> presentation. (*=equal contribution) </p>

<li><p><a href="https://doi.org/10.1109/JSAIT.2023.3262689">Local Geometry of Nonconvex Spike Deconvolution from Low-Pass Measurements</a> <a href="https://arxiv.org/abs/2208.10073">[Arxiv]</a>
<br>M. Ferreira Da Costa and Y. Chi, <i>IEEE Journal on Selected Areas in Information Theory</i>, vol. 4, pp. 1-15, 2023.</p> 

<li> <p><a href="https://doi.org/10.1093/imaiai/iaac034">Breaking the Sample Complexity Barrier to Regret-Optimal Model-free Reinforcement Learning</a> <a href="https://arxiv.org/pdf/2110.04645.pdf">[Arxiv]</a> 
<br>G. Li, L. Shi, Y. Chen, and Y. Chi, <i>Information and Inference: A Journal of the IMA</i>, vol. 12, no. 2, pp. 969-1043, 2023. Short version at NeurIPS 2021 as a <b>spotlight</b> presentation. </p>

<li><p><a href="https://openreview.net/forum?id=cOQH8YO255&noteId=ldVpt7USiW">The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model</a> <a href="https://arxiv.org/abs/2305.16589">[Arxiv]</a>
<br>L. Shi, G. Li, Y. Wei, Y. Chen, M. Geist, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023.

<li><p><a href="https://openreview.net/forum?id=Nd3FennRJZ">Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning</a> <a href="https://arxiv.org/abs/2305.10282">[Arxiv]</a>
<br>G. Li*, W. Zhan*, J. D. Lee, Y. Chi, and Y. Chen, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023. (*=equal contribution) </p> 

<li><p><a href="https://openreview.net/forum?id=bTL5SNOpfa">Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation</a> <a href="https://arxiv.org/abs/2307.07907">[Arxiv]</a> <a href="https://sites.google.com/view/causaldro">[Website]</a>
<br>W. Ding*, L. Shi*, Y. Chi, and D. Zhao, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023. (*=equal contribution) </p> 

<li><p><a href="https://openreview.net/forum?id=Uc5yyiytR1">Identification of Nonlinear Latent Hierarchical Models</a> <a href="https://arxiv.org/abs/2306.07916">[Arxiv]</a>
<br>L. Kong, B. Huang, F. Xie, E. Xing, Y. Chi, and K. Zhang, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023.

<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-43421-1_27">Offline Reinforcement Learning with On-Policy Q-Function Regularization</a> <a href="https://arxiv.org/abs/2307.13824">[Arxiv]</a>
<br>L. Shi, R. Dadashi, Y. Chi, P. S. Castro, and M. Geist, <i>European Conference on Machine Learning (ECML)</i>, 2023.

<li><p><a href="https://proceedings.mlr.press/v202/xu23o/xu23o.pdf">The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing</a> <a href="https://arxiv.org/abs/2302.01186">[Arxiv]</a>
<br>X. Xu, Y. Shen, Y. Chi, and C. Ma, <i>International Conference on Machine Learning (ICML)</i>, 2023.</p> 

<li><p><a href="https://openreview.net/pdf?id=yE1_GpmDOPL">A Trajectory is Worth Three Sentences: Multimodal Transformer for Offline Reinforcement Learning</a> 
<br>Y. Wang, M. Xu, L. Shi, and Y. Chi, <i>Conference on Uncertainty in Artificial Intelligence (UAI)</i>, 2023.</p> 

<li><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Autoencoders_via_Hierarchical_Latent_Variable_Models_CVPR_2023_paper.pdf">Understanding Masked Autoencoders via Hierarchical Latent Variable Models</a> <a href="https://arxiv.org/pdf/2306.04898.pdf">[Arxiv]</a>
<br>L. Kong, M. Q. Ma, G. Chen, E. Xing, Y. Chi, L.-P. Morency, and K. Zhang, <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2023, <b>highlight</b> presentation. </p>

<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095485">Deep Unfolded Tensor Robust PCA with Self-supervised Learning</a> <a href="https://arxiv.org/abs/2212.11346">[Arxiv]</a> <a href="https://github.com/hdong920/Tensor_RPCA_ScaledGD">[Code]</a>
<br>H. Dong, M. Shah, S. Donegan, and Y. Chi, <i>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2023. </p> 

<li><p><a href="https://openreview.net/forum?id=vPXp7K_Yhre">Asynchronous Gradient Play in Zero-Sum Multi-agent Games</a> <a href="https://arxiv.org/abs/2211.08980">[Arxiv]</a>
<br>R. Ao, S. Cen, and Y. Chi, <i>International Conference on Learning Representations (ICLR)</i>, 2023.  (Authors are listed alphabetically.)

<li><p><a href="https://openreview.net/forum?id=bRwBpKrNzF7">Faster Last-iterate Convergence of Policy Optimization in Zero-Sum Markov Games</a> <a href="https://arxiv.org/abs/2210.01050">[Arxiv]</a>
<br>S. Cen, Y. Chi, S. Du, and L. Xiao, <i>International Conference on Learning Representations (ICLR)</i>, 2023.  (Authors are listed alphabetically.)


<h3> 2022 </h3> 

<li><p><a href="https://doi.org/10.1137/21M1450677">DESTRESS: Computation-Optimal and Communication-Efficient Decentralized Nonconvex Finite-Sum Optimization</a> <a href="https://arxiv.org/pdf/2110.01165.pdf">[Arxiv]</a> <a href="https://github.com/liboyue/Network-Distributed-Algorithm">[Code]</a>
<br>B. Li, Z. Li, and Y. Chi, <i>SIAM Journal on Mathematics of Data Science</i>, vol. 4, no. 3, pp. 1031-1051, 2022. Short version at OPT 2021 as a <b>spotlight</b> presentation. </p>

<li><p><a href="https://pubsonline.informs.org/doi/abs/10.1287/opre.2021.2151">Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization</a> <a href="https://arxiv.org/pdf/2007.06558.pdf">[Arxiv]</a> <a href="https://colab.research.google.com/drive/1LQ0hYxIuPintMNnAqcf9ZOyKpJNnxx8B?usp=sharing">[Code]</a>
<br>S. Cen, C. Cheng, Y. Chen, Y. Wei, and Y. Chi, <i>Operations Research</i>, vol. 70, no. 4, pp. 2563-2578, 2022.
<br><b>INFORMS George Nicholson Student Paper Competition Finalist</b> </p>

<li><p><a href="https://www.nature.com/articles/s41597-022-01545-6">A Large Collection of Real-world Pediatric Sleep Studies</a> <a href="https://arxiv.org/abs/2102.13284">[Arxiv]</a> <a href="https://www.sleepdata.org/datasets/nchsdb">[NSRR]</a> <a href="https://physionet.org/content/nch-sleep/">[PhysioNet]</a> <a href="https://github.com/liboyue/sleep_study ">[Code]</a>
<br>H. Lee, B. Li, S. DeForte, M. Splaingard, Y. Huang, Y. Chi, and S. Linwood, <i>Scientific Data</i>, 2022.

<li><p><a href="https://jmlr.org/papers/v23/21-1390.html">Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements</a> <a href="https://arxiv.org/pdf/2104.14526.pdf">[Arxiv]</a> <a href="https://github.com/Titan-Tong/ScaledGD">[Code]</a>
<br>T. Tong, C. Ma, A. Prater-Bennette, E. Tripp, and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 23, no. 163, pp. 1-77, 2022. Short version at AISTATS 2022.</p> 

<li> <p><a href="https://ieeexplore.ieee.org/document/9570295">Sample Complexity of Asynchronous Q-Learning:
Sharper Analysis and Variance Reduction</a> <a href="https://arxiv.org/pdf/2006.03041.pdf">[Arxiv]</a> 
<br>G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen, <i>IEEE Trans. on Information Theory</i>, vol. 68, no. 1, pp. 448-473, 2022. Short version at NeurIPS 2020.</p>

<li> <p><a href="https://openreview.net/forum?id=W8nyVJruVg">Minimax-Optimal Multi-Agent RL in Markov Games With a Generative Model</a> <a href="https://arxiv.org/abs/2208.10458">[Arxiv]</a> 
<br>G. Li, Y. Chi, Y. Wei, and Y. Chen,  <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2022, <b>oral</b> presentation.  </p> 

<li> <p><a href="https://openreview.net/forum?id=I47eFCKa1f3">BEER: Fast O(1/T) Rate for Decentralized Nonconvex
Optimization with Communication Compression</a> <a href="https://arxiv.org/pdf/2201.13320.pdf">[Arxiv]</a> <a href="https://github.com/liboyue/beer">[Code]</a>
<br>H. Zhao, B. Li, Z. Li, P. Richtarik, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2022. 

<li><p><a href="https://openreview.net/forum?id=tz1PRT6lfLe">SoteriaFL: A Unified Framework for Private Federated Learning with Communication Compression</a> <a href="https://arxiv.org/abs/2206.09888">[Arxiv]</a> <a href="https://github.com/haoyuzhao123/soteriafl">[Code]</a>
<br>Z. Li, H. Zhao, B. Li, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2022. 

<li><p><a href="https://proceedings.mlr.press/v162/shi22c.html">Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity</a> <a href="https://arxiv.org/abs/2202.13890">[Arxiv]</a> 
<br>L. Shi, G. Li, Y. Wei, Y. Chen, and Y. Chi, <i>International Conference on Machine Learning (ICML)</i>, 2022.   

<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9993175">Independent Natural Policy Gradient Methods for Potential Games: Finite-time Global Convergence with Entropy Regularization</a> <a href="https://arxiv.org/pdf/2204.05466.pdf">[Arxiv]</a> <b>[Invited Paper]</b>
<br>S. Cen, F. Chen, and Y. Chi, <i>IEEE Conference on Decision and Control (CDC)</i>, 2022.</p>

<li><p><a href="https://ieeexplore.ieee.org/document/10051851">Harvesting Curvatures for Communication-Efficient Distributed Optimization</a>
<br>D. Cardoso, B. Li, Y. Chi, and J. Xavier, <i>Asilomar Conference on Signals, Systems, and Computers (Asilomar)</i>, 2022.</p>

<li><p><a href="https://doi.org/10.1109/PAST49659.2022.9975097">Zoom Out: Abstractions for Efficient Radar Algorithms on COTS Architectures</a>
<br>T. Low, Y. Chi, J. Hoe, S. Kumar, A. Prabhakara, L. Shi, U. Sridhar, N. Tukanov, C. Wang, and Y. Wu, <i>IEEE International Symposium on Phased Array Systems and Technology (PAST)</i>, 2022. </p>

<li><p><a href="https://ieeexplore.ieee.org/document/10027676">Active Heterogeneous Graph Neural Networks with Per-step Meta-Q-Learning</a>
<br>Y. Zhang, Y. Xia, Y. Zhu, Y. Chi, L. Ying and H. Tong, <i>IEEE International Conference on Data Mining (ICDM)</i>, 2022.</p> 

<li><p><a href="https://doi.org/10.1609/aaai.v36i8.20897">Batch Active Learning with Graph Neural Networks via Multi-Agent Deep Reinforcement Learning</a>  
<br>Y. Zhang, H. Tong, Y. Xia, Y. Zhu, Y. Chi, and L. Ying, <i>AAAI Conference on Artificial Intelligence (AAAI)</i>, 2022.</p> 

<li><p><a href="https://doi.org/10.1109/ICASSP43922.2022.9746007">Privacy-Preserving Federated Multi-task Linear Regression: A One-Shot Linear Mixing Approach Inspired by Graph Regularization</a> 
<br>H. Lee, A. Bertozzi, J. Kovacevic, and Y. Chi, <i>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2022.</p> 

<li><p><a href="https://doi.org/10.1109/ICASSP43922.2022.9746705">Accelerating Ill-Conditioned Robust Low-Rank Tensor Regression</a> 
<br>T. Tong, C. Ma, and Y. Chi, <i>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2022.</p> 




<h3> 2021 </h3> 

<li><p><a href="https://jmlr.org/papers/volume22/20-1067/20-1067.pdf">Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent</a> <a href="https://arxiv.org/pdf/2005.08898.pdf">[Arxiv]</a> <a href="https://github.com/Titan-Tong/ScaledGD">[Code]</a>
<br>T. Tong, C. Ma, and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 22, no. 150, pp. 1-63, 2021.</p> 

<li><p><a href="https://ieeexplore.ieee.org/document/9410615">Manifold Gradient Descent Solves Multi-channel Sparse Blind Deconvolution Provably and Efficiently</a> <a href="https://arxiv.org/pdf/1911.11167.pdf">[Arxiv]</a> <a href="http://www.andrew.cmu.edu/user/laixis/laixi_photo/publication3_msbd/msbd_nonconvex_compressed.pdf">[Slides]</a>
<br>L. Shi and Y. Chi, <i>IEEE Trans. on Information Theory</i>, vol. 67, no. 7, pp. 4784-4811, 2021. Short version at ICASSP 2020. </p> 

<li><p><a href="https://ieeexplore.ieee.org/document/9398573">Low-Rank Matrix Recovery with Scaled Subgradient Methods: Fast and Robust Convergence Without the Condition Number</a> <a href="https://arxiv.org/pdf/2010.13364.pdf">[Arxiv]</a>  
<br>T. Tong, C. Ma, and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 69, pp. 2396-2409, 2021. Short version received <b>Audience Choice Award</b> at DSLW 2021.</p> 

<li><p><a href="http://dx.doi.org/10.1214/20-AOS1986">Subspace Estimation from Unbalanced and Incomplete Data Matrices: $\ell_{2,\infty}$ Statistical Guarantees</a> <a href="https://arxiv.org/abs/1910.04267">[Arxiv]</a>
<br>C. Cai, G. Li, Y. Chi, H. V. Poor, and Y. Chen, <i>Annals of Statistics</i>, vol. 49, no. 2, pp. 944-967, 2021.</p>

<li><p><a href="https://ieeexplore.ieee.org/document/9317779">Nonconvex Matrix Factorization from Rank-One Measurements</a> <a href="https://arxiv.org/pdf/1802.06286.pdf">[Arxiv]</a>  
<br>Y. Li, C. Ma, Y. Chen, and Y. Chi, <i>IEEE Trans. on Information Theory</i>, vol. 67, no. 3, pp. 1928-1950, 2021. Short version at AISTATS 2019. </p>

<li><p><a href="https://ieeexplore.ieee.org/document/9321745">Beyond Procrustes: Balancing-free Gradient Descent for Asymmetric Low-Rank Matrix Sensing</a> <a href="https://arxiv.org/abs/2101.05113">[Arxiv]</a>  
<br>C. Ma, Y. Li, and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 69, pp. 867-877, 2021. Short version at Asilomar 2019.</p> 
 
<li><p><a href="https://ieeexplore.ieee.org/document/9296231">Compressed Super-Resolution of Positive Sources</a> <a href="https://arxiv.org/pdf/2010.10461">[Arxiv]</a>
<br>M. Ferreira Da Costa and Y. Chi, <i>IEEE Signal Processing Letters</i>, vol. 28, pp. 56-60, 2021.</p>

<li><p><a href="https://openreview.net/forum?id=2e_VWzcU4j7">Sample-Efficient Reinforcement Learning Is Feasible for Linearly Realizable MDPs with Limited Revisiting</a> <a href="https://arxiv.org/pdf/2105.08024.pdf">[Arxiv]</a> 
<br>G. Li, Y. Chen, Y. Chi, Y. Gu, and Y. Wei, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2021.</p> 

<li><p><a href="https://ieeexplore.ieee.org/document/9506021">Plug-and-Play Image Reconstruction Meets Stochastic Variance-Reduced Gradient Methods</a>
<br>V. Monardo, A. Iyer, S. Donegan, M. De Graef, and Y. Chi, <i>IEEE International Conference on Image Processing (ICIP)</i>, 2021.</p>

<h3> 2020 </h3> 


<li><p><a href="https://epubs.siam.org/doi/pdf/10.1137/19M1290000">Noisy Matrix Completion: Understanding Statistical Guarantees for Convex Relaxation via Nonconvex Optimization</a> <a href="https://arxiv.org/pdf/1902.07698.pdf">[Arxiv]</a>
<br>Y. Chen, Y. Chi, J. Fan, C. Ma and Y. Yan, <i>SIAM Journal on Optimization</i>, vol. 30, no. 4, pp. 3098-3121, 2020. (Authors are listed alphabetically.)

<li><p><a href="https://ieeexplore.ieee.org/document/9090204">On the Stable Resolution Limit of Total Variation Regularization for Spike Deconvolution</a> <a href=" https://arxiv.org/pdf/1910.01629.pdf">[Arxiv]</a>
<br>M. Ferreira Da Costa and Y. Chi, <i>IEEE Trans. on Information Theory</i>, vol. 66, no. 11, pp. 7237-7252, 2020. Short version at CISS 2020.</p> 

<li><p><a href="https://jmlr.org/papers/v21/20-210.html">Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction</a> <a href="https://arxiv.org/pdf/1909.05844.pdf">[Arxiv]</a> <a href="https://github.com/liboyue/Network-Distributed-Algorithm">[Code]</a>
<br>B. Li, S. Cen, Y. Chen, and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 21, no. 180, pp. 1-51, 2020. Short version at AISTATS 2020.</p>

<li><p><a href="https://ieeexplore.ieee.org/document/9136786">Learning Latent Features with Pairwise Penalties in Low-Rank Matrix Completion</a> <a href="http://arxiv.org/abs/1802.05821">[Arxiv]</a>
<br>K. Ji, J. Tan, J. Xu, and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 68, pp. 4210-4225, 2020. Short version at SAM 2020.</p>

<li><p><a href="https://ieeexplore.ieee.org/document/9127115/">Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data</a> <a href="https://arxiv.org/pdf/1905.12648.pdf">[Arxiv]</a>
<br>S. Cen, H. Zhang, Y. Chi, W. Chen and T.-Y. Liu, <i>IEEE Trans. on Signal Processing</i>, vol. 68, pp. 3976-3989, 2020.

<li><p><a href="https://ieeexplore.ieee.org/document/9089304">Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy</a> <a href="https://arxiv.org/pdf/1802.06463.pdf">[Arxiv]</a>
 <br> H. Fu, Y. Chi, and Y. Liang, <i>IEEE Trans. on Signal Processing</i>, vol. 68, pp. 3225-3235, 2020. Short version at ISIT 2019.</p>
 

<li><p><a href="https://doi.org/10.1093/imaiai/iaz009">Nonconvex Low-Rank Matrix Recovery with Arbitrary Outliers via Median-Truncated Gradient Descent</a> <a href="https://arxiv.org/abs/1709.08114">[Arxiv]</a>
<br>Y. Li, Y. Chi, H. Zhang, and Y. Liang, <i>Information and Inference: A Journal of the IMA</i>, vol. 9, no. 2, pp. 289-325, 2020. Short version at SampTA 2017.</p>


<li><p><a href="http://doi.org/10.1007/s10208-019-09429-9">Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind Deconvolution</a> <a href="https://arxiv.org/pdf/1711.10467.pdf">[Arxiv]</a>
<br>C. Ma, K. Wang, Y. Chi, and Y. Chen, <i>Foundations of Computational Mathematics</i>, vol. 20, pp. 451-632, 2020. Short version at ICML 2018.
<br><b>2024 SIAM Activity Group on Imaging Science Best Paper Prize</b></p>


<li><p><a href="https://doi.org/10.1016/j.automatica.2019.108715">Analytical Convergence Regions of Accelerated Gradient Descent in Nonconvex Optimization under Regularity Condition</a> <a href="https://arxiv.org/pdf/1810.03229.pdf">[Arxiv]</a>
<br>H. Xiong, Y. Chi, B. Hu, and W. Zhang, <i>Automatica</i>, vol. 113, pp. 108715, 2020. Short version at MTNS 2018.</p>

<li><p><a href="https://ieeexplore.ieee.org/document/8926407">Vector-Valued Graph Trend Filtering with Non-Convex Penalties</a> <a href="https://arxiv.org/pdf/1905.12692.pdf">[Arxiv]</a>
 <a href="https://github.com/HarlinLee/nonconvex-GTF-public">[Code]</a>
<br>R. Varma*, H. Lee*, J. Kovacevic and Y. Chi, <i>IEEE Trans. on Signal Processing over Networks</i>, vol. 6, no. 1, pp. 48-62, 2020. Short version at ICASSP 2019. (*=equal contribution)</p>  
 

<h3> Before 2019 (Selected Journal Papers) </h3>  

<li><p><a href="https://link.springer.com/article/10.1007/s10107-019-01363-6">Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval</a> <a href="https://arxiv.org/pdf/1803.07726.pdf">[Arxiv]</a>
<br>Y. Chen, Y. Chi, J. Fan and C. Ma, <i>Mathematical Programming</i>, vol. 176, no. 1, pp. 5-37, 2019. (Authors are listed alphabetically.)</p>

<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8675477">Low-Rank Structured Covariance Matrix Estimation</a> <a href="code/CovCode.zip">[Code]</a>
<br>A. P. Shikhaliev, L. C. Potter and Y. Chi, <i>IEEE Signal Processing Letters</i>, vol. 26, no. 5, pp. 700-704, 2019.</p>

<li> <p><a href="http://www.sciencedirect.com/science/article/pii/S1063520317300234">Stable Separation and Super-Resolution of Mixture Models</a> <a href="http://arxiv.org/abs/1506.07347">[Arxiv]</a>
<br> Y. Li and Y. Chi, <i>Applied and Computational Harmonic Analysis</i>, vol. 46, no. 1, pp. 1-39, 2019. Short versions at ISIT 2015 and SampTA 2015.</p>

<li> <p><a href="https://ieeexplore.ieee.org/document/8454912/">Low-Rank Matrix Completion</a> 
<br> Y. Chi, <i>IEEE Signal Processing Magazine</i>, vol. 35, no. 5, pp. 178-181, 2018. </p>

<li> <p><a href="https://ieeexplore.ieee.org/document/8386800/">Median-Truncated Nonconvex Approach for Phase Retrieval with Outliers</a> <a href="https://arxiv.org/pdf/1603.03805.pdf">[Arxiv]</a>
<br> H. Zhang, Y. Chi and Y. Liang, <i>IEEE Trans. on Information Theory</i>,  vol. 64, no. 11, pp. 7287-7310, 2018. Short version at ICML 2016.</p>


<li> <p><a href="https://ieeexplore.ieee.org/document/8340173/">Quantized Spectral Compressed Sensing: Cramer-Rao Bounds and Recovery Algorithms</a> <a href="https://arxiv.org/abs/1710.03654">[Arxiv]</a>
<br> H. Fu and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 66, no. 12, pp. 3268-3279, 2018. Short version at Asilomar 2017.</p>

<li> <p><a href="http://ieeexplore.ieee.org/document/8141954/">Stochastic Approximation and Memory-Limited Subspace Tracking for Poisson Streaming Data</a> 
<br> L. Wang and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 66, no. 4, pp. 1051-1064, 2018. Short version at CAMSAP 2017.</p> 

<li> <p><a href="http://www.jmlr.org/papers/volume18/16-572/16-572.pdf">A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms</a> <a href="https://github.com/hubevan/reshaped-Wirtinger-flow">[Code]</a>
<br> H. Zhang, Y. Zhou, Y. Liang and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 18, no. 141, pp. 1-35, 2017.</p>

<li> <p><a href="http://dx.doi.org/10.1109/TSP.2017.2712127">Subspace Learning From Bits</a> <a href="http://arxiv.org/abs/1407.6288">[Arxiv]</a>
<br> Y. Chi and H. Fu, <i>IEEE Trans. on Signal Processing</i>, vol. 65, no. 17, pp. 4429-4442, 2017. Short versions at GlobalSIP 2014 and Asilomar 2016.</p>

<li> <p><a href="http://dx.doi.org/10.1109/TCI.2017.2699425">Super-Resolution Image Reconstruction for High-Density 3D Single-Molecule Microscopy</a> <a href="papers/SMLM_Reconstruction.zip">[Code]</a>
<br> J. Huang, M. Sun, J. Ma and Y. Chi, <i>IEEE Trans. on Computational Imaging</i>, vol. 3, no. 4, pp. 763-773, 2017. Short version at ISBI 2016.</p>

<li> <p><a href="http://dx.doi.org/10.1109/TSP.2016.2620109">Low-Rank Positive Semidefinite Matrix Recovery from Corrupted Rank-One Measurements</a> <a href="http://arxiv.org/abs/1602.02737">[Arxiv]</a>
<br> Y. Li, Y. Sun and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 65, no. 2, pp. 397-408, 2017. Short version at ICASSP 2016. </p>
  
<li> <p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7536657">Blind Deconvolution from Multiple Sparse Inputs</a> 
<br> L. Wang and Y. Chi, <i>IEEE Signal Processing Letters</i>, vol. 23, no. 10, pp. 1384-1388, 2016.</p>
<li> <p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7509600">Kaczmarz Method for Solving Quadratic Equations</a>
<br> Y. Chi and Y. M. Lu, <i>IEEE Signal Processing Letters</i>, vol. 23, no. 9, pp. 1183 - 1187, 2016.</p>
<li> <p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7435246">Guaranteed Blind Sparse Spikes Deconvolution via Lifting and Convex Optimization</a> <a href="http://arxiv.org/abs/1506.02751">[Arxiv]</a> <a href="code/atomiclift.m">[Code]</a>
<br> Y. Chi, <i>IEEE Journal of Selected Topics in Signal Processing - Special Issue on Structured Matrices in Signal and Data Processing</i>, vol. 10, no. 4, pp. 782 - 794, 2016. Short versions at Asilomar 2015 and ICASSP 2016.</p>
<li> <p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7313018">Off-the-Grid Line Spectrum Denoising and Estimation with Multiple Measurement Vectors</a> <a href="http://arxiv.org/abs/1408.2242">[Arxiv]</a> <a href="code/atomic_mmv.zip">[Code]</a>
<br> Y. Li and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 64, pp. 1257 - 1269, 2016. Short versions at ICASSP 2014 and SSP 2014.</p> 
 
<li>  <p><a href="https://www.osapublishing.org/ol/abstract.cfm?URI=ol-40-13-2989">Fast Two-dimensional Super-resolution Image Reconstruction Algorithm for Ultra-high Emitter Density</a> <a href="code/SMLM_Reconstruction.zip">[Code]</a>
<br> J. Huang, K. Gumpper, Y. Chi, M. Sun and J. Ma, <i>Optics Letters</i>, vol. 40, pp. 2989 - 2992, 2015.</p>
<li> <p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7101247">Exact and Stable Covariance Estimation from Quadratic Sampling via Convex Programming </a> <a href="http://arxiv.org/abs/1310.0807">[Arxiv]</a>
<br>Y. Chen, Y. Chi and A. J. Goldsmith, <i>IEEE Trans. on Information Theory</i>, vol. 61, pp. 4034 - 4059, 2015. Short versions at ISIT 2014 and ICASSP 2014.</p>
<li> <p><a href="http://www.opticsinfobase.org/boe/abstract.cfm?uri=boe-6-3-902">3D Multifocus Astigmatism and Compressed Sensing (3D MACS) Based Superresolution Reconstruction</a> <a href="code/SMLM_Reconstruction.zip">[Code]</a>
<br>J. Huang, M. Sun, K. Gumpper, Y. Chi and J. Ma, <i>Biomedical Optics Express</i>, vol. 6, pp. 902 - 917, 2015.</p>
<li> <p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998075">Compressive Two-Dimensional Harmonic Retrieval via Atomic Norm Minimization</a> <a href="code/superres_2datomic.m">[Code]</a>
<br>Y. Chi and Y. Chen, <i>IEEE Trans. on Signal Processing</i>, vol. 63, pp. 1030 - 1042, 2015. Short version at Asilomar 2013.</p>
  
<li> <p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6867345">Robust Spectral Compressed Sensing via Structured Matrix Completion</a> <a href="http://arxiv.org/abs/1304.8126">[Arxiv]</a> <a href="code/EMaC_codes.zip">[Code]</a>
<br>Y. Chen and Y. Chi, <i>IEEE Trans. on Information Theory</i>, vol. 60, pp. 6576-6601, 2014. Short version at ICML 2013 and <b>Best Student Paper Award Finalist</b> at SPARS 2013.</p>

<li> <p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678501">Classification and Boosting with Multiple Collaborative Representations</a>
<br>Y. Chi and F. Porikli, <i>IEEE Trans. on Pattern Analysis and Machine Intelligence</i>, vol. 36, pp. 1519 - 1531, 2014. Short version at CVPR 2012.</p>
 
<li> <p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6605610">PETRELS:  Parallel Subspace Estimation and Tracking using Recursive Least Squares from Partial Observations</a> <a href="code/petrels.zip">[Code]</a>
<br>Y. Chi, Y. C. Eldar, and R. Calderbank, <i>IEEE Trans. on Signal Processing</i>, vol. 61, pp. 5947 - 5959, 2013. Short version received <b>Best Student Paper Award</b> at ICASSP 2012.</p>
 
<li> <p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5710590">Sensitivity of Basis Mismatch to Compressed Sensing</a> 
<br>Y. Chi, L. L. Scharf, A. Pezeshki and R. Calderbank, <i>IEEE Trans. on Signal Processing</i>, vol. 59, pp. 2182 - 2195, 2011. Short versions at ICASSP 2010 and DASP 2009.
<br><b>2013 IEEE Signal Processing Society Young Author Best Paper Award</b></p>

</ul>


  
<h2> Dissertation</h2>
<p><br></p>
<ul>
<li> <a href="dissertation/Chi_dissertation.pdf">Exploitation of Geometry in Signal Processing and Sensing</a>, Ph. D. Dissertation, Princeton University, Sep. 2012. </a></p>
</li>
</ul>
 
  


<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
