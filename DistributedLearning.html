
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yuejie Chi</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><img class="menu" src="yale_logo.png" width="100px" align="center"></div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publications.html">Papers</a></div>
<div class="menu-item"><a href="talks.html">Talks</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="news.html">News</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yuejie Chi</h1>
</div>
 

<h2> Resource-Efficient Decentralized and Federated Learning </h2>
<p></p>


<p><img src="photos/distributed_optimization.png" width="250" style="float:right; margin:0px 10px 5px 10px;" /></p>
 <p> Distributed optimization has been a classic topic, yet is attracting significant interest recently in machine learning due to its numerous applications such as distributed training, multi-agent learning, federated optimization, and so on. Often, the scale of modern datasets has exceeded the capacity of a single machine, and privacy and communication constraints prevent information sharing in a centralized manner and necessitates distributed infrastructures. Broadly speaking, there are two types of distributed settings: a distributed/federated setting, where a parameter server aggregates and shares parameters across all agents; and a decentralized/network setting, where each agent only aggregates and shares parameters with its neighbors over a network topology. The canonical problem of empirical risk minimization in the distributed setting leads to intriguing trade-offs between computation and communication that are not well understood; moreover, data unbalancedness and heterogeneity across agents poses additional challenges in both algorithmic convergence and statistical efficacy, often exacerbated by additional bandwidth and privacy constraints.  
 </p>

<h3> Overview </h3> 

<ul> 
<li> <p><a href="talks/FedOpt_tutorial.pdf">Advances in Federated Optimization: Efficiency, Resiliency, and Privacy</a> 
<br>Y. Chi and Z. Li, <i>ICASSP tutorial</i>, 2023. </p>

</ul>

<h3>Communication-Privacy Trade-offs</h3>

<ul>

<li><p><a href="https://openreview.net/forum?id=9bB1FJSKKS">Characterizing the Accuracy-Communication-Privacy Trade-off in Distributed Stochastic Convex Optimization</a> <a href="https://arxiv.org/abs/2501.03222">[Arxiv]</a>
<br>S. Salgia, N. Pavlovic, Y. Chi and Q. Zhao, <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2025. </p> 
 
<li><p><a href="https://doi.org/10.1109/JSTSP.2025.3526081">Convergence and Privacy of Decentralized Nonconvex Optimization with Gradient Clipping and Communication Compression</a> <a href="https://arxiv.org/abs/2305.09896">[Arxiv]</a>
<br>B. Li and Y. Chi, <i>IEEE Journal of Selected Topics in Signal Processing</i>, vol. 19, no. 1, pp. 273-282, 2025.  </p> 

<li><p><a href="https://openreview.net/forum?id=tz1PRT6lfLe">SoteriaFL: A Unified Framework for Private Federated Learning with Communication Compression</a> <a href="https://arxiv.org/abs/2206.09888">[Arxiv]</a> <a href="https://github.com/haoyuzhao123/soteriafl">[Code]</a>
<br>Z. Li, H. Zhao, B. Li, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2022. 

</ul>


<h3> Communication-Efficient Federated and Decentralized Optimization</h3>

<ul>

<li><p><a href="https://doi.org/10.1109/TSIPN.2025.3539004">Communication-Efficient Federated Optimization over Semi-Decentralized Networks</a> <a href="https://arxiv.org/abs/2311.18787">[Arxiv]</a>  
<br>H. Wang and Y. Chi, <i>IEEE Trans. on Signal and Information Processing over Networks</i>, vol. 11, pp. 147-160, 2025. Short version at ICASSP 2024.  </p> 

<li><p><a href="https://proceedings.mlr.press/v238/chen24d/chen24d.pdf">Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression</a> <a href="https://arxiv.org/abs/2310.19059">[Arxiv]</a>
<br>S. Chen, Z. Li, and Y. Chi, <i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2024. 

<li> <p><a href="https://openreview.net/forum?id=I47eFCKa1f3">BEER: Fast O(1/T) Rate for Decentralized Nonconvex Optimization with Communication Compression</a> <a href="https://arxiv.org/pdf/2201.13320.pdf">[Arxiv]</a> <a href="https://github.com/liboyue/beer">[Code]</a>
<br>H. Zhao, B. Li, Z. Li, P. Richtarik, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2022. 

<li> <p><a href="https://doi.org/10.1137/21M1450677">DESTRESS: Computation-Optimal and Communication-Efficient Decentralized Nonconvex Finite-Sum Optimization</a> <a href="https://arxiv.org/pdf/2110.01165.pdf">[Arxiv]</a> <a href="https://github.com/liboyue/Network-Distributed-Algorithm">[Code]</a>
<br>B. Li, Z. Li, and Y. Chi, <i>SIAM Journal on Mathematics of Data Science</i>, vol. 4, no. 3, pp. 1031-1051, 2022. Short version at OPT 2021 as a <b>spotlight</b> presentation. </p>

<li> <p><a href="https://jmlr.org/papers/v21/20-210.html">Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction</a> <a href="https://arxiv.org/pdf/1909.05844.pdf">[Arxiv]</a> <a href="https://github.com/liboyue/Network-Distributed-Algorithm">[Code]</a>
<br>B. Li, S. Cen, Y. Chen, and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 21, no. 180, pp. 1-51, 2020. Short version at AISTATS 2019. </p>

<li> <p><a href="https://ieeexplore.ieee.org/document/9127115/">Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data</a> <a href="https://arxiv.org/pdf/1905.12648.pdf">[Arxiv]</a>
<br>S. Cen, H. Zhang, Y. Chi, W. Chen and T.-Y. Liu, <i>IEEE Trans. on Signal Processing</i>, vol. 68, pp. 3976-3989, 2020. </p>

</ul>

<h3> Vertical Federated Learning </h3>

<ul>

<li><p><a href="https://openreview.net/forum?id=OXi1FmHGzz">Vertical Federated Learning with Missing Features During Training and Inference</a> <a href="https://arxiv.org/abs/2410.22564">[Arxiv]</a>
<br>P. Valdeira, S. Wang, and Y. Chi, <i>International Conference on Learning Representations (ICLR)</i>, 2025. </p> 

<li><p><a href="https://doi.org/10.1109/TSP.2025.3540655">Communication-efficient Vertical Federated Learning via Compressed Error Feedback</a> <a href="https://arxiv.org/abs/2406.14420">[Arxiv]</a> 
<br>P. Valdeira, J. Xavier, C. Soares, and Y. Chi, <i>IEEE Trans. on Signal Processing</i>, vol. 73, pp. 1065-1080, 2025. Short version at EUSIPCO 2024 as an <b>invited</b> paper.  </p> 

<li><p><a href="https://arxiv.org/abs/2309.09977">A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning</a> <a href="https://arxiv.org/abs/2309.09977">[Arxiv]</a>
<br>P. Valdeira, Y. Chi, C. Soares, and J. Xavier, <i>preprint</i>. 

</ul>

<h3> Federated Reinforcement Learning </h3>

<ul>

<li><p><a href="https://jmlr.org/papers/volume26/24-0579/24-0579.pdf">The Blessing of Heterogeneity in Federated Q-Learning: Linear Speedup and Beyond</a> <a href="https://arxiv.org/abs/2305.10697">[Arxiv]</a>
<br>J. Woo, G. Joshi, and Y. Chi, <i>Journal of Machine Learning Research</i>, vol. 26, no. 26, pp. 1-85, 2025. Short version at ICML 2023. </p> 

<li><p><a href="https://openreview.net/forum?id=6YIpvnkjUK">The Sample-Communication Complexity Trade-off in Federated Q-Learning</a> <a href="https://arxiv.org/abs/2408.16981">[Arxiv]</a>
<br>S. Salgia and Y. Chi,  <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2024, <b>oral</b> presentation.  </p> 

<li><p><a href="https://openreview.net/forum?id=DUFD6vsyF8">Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning</a> <a href="https://arxiv.org/abs/2311.00201">[Arxiv]</a>
<br>T. Yang, S. Cen, Y. Wei, Y. Chen, and Y. Chi, <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2024. </p> 

<li><p><a href="https://openreview.net/pdf?id=LIPGadocTe">Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices</a> <a href="https://arxiv.org/abs/2402.05876">[Arxiv]</a>
<br>J. Woo, L. Shi, G. Joshi, and Y. Chi, <i> International Conference on Machine Learning (ICML)</i>, 2024.  </p> 

</ul>

<p><br /><br /></p>
<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>